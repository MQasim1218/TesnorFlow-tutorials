{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std Libraries...\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "# Data manipulation libraries...\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "\n",
    "# Deep-Learning libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "from tensorflow.keras import layers as lyrs, optimizers as opts, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Downloading Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **`Don't run this cell, it downloads data which is already done`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\n",
    "    \"aclImdb_v1\", url,\n",
    "    untar=True, cache_dir='.',\n",
    "    cache_subdir=''\n",
    ")\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "print(os.listdir(dataset_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **Checking a sample file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labeledBow.feat', 'neg', 'pos', 'unsup', 'unsupBow.feat', 'urls_neg.txt', 'urls_pos.txt', 'urls_unsup.txt']\n",
      "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "print(os.listdir(train_dir))\n",
    "\n",
    "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
    "with open(sample_file) as f:\n",
    "    print(f.read())\n",
    "    \n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **For data-preprocessing, it has to be passed onto library that expects a file structure like:**\n",
    "\n",
    "* main_directory/\n",
    "* ...class_a/\n",
    "* ......a_text_1.txt\n",
    "* ......a_text_2.txt\n",
    "* ...class_b/\n",
    "* ......b_text_1.txt\n",
    "* ......b_text_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Data For training**\n",
    "> ##### **Dividing train data into training and validation data using `text_dataset_from_directory`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "### FunctionParameters ###\n",
    "bs = 32\n",
    "s = 42\n",
    "\n",
    "# this is the trainig set...\n",
    "RawTrainDataset = tfk.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=bs, seed=s,\n",
    "    validation_split=0.2, subset='training'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Label `0` corresponds to `neg`**\n",
    "##### **Label `1` corresponds to `pos`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:: b\"I'm a Christian who generally believes in the theology taught in Left Behind. That being said, I think Left Behind is one of the worst films I've seen in some time.<br /><br />To have a good movie, you need to have a well-written screenplay. Left Behind fell woefully short on this. For one thing, it radically deviates from the book. Sometimes this is done to condense a 400-page novel down to a two-hour film, but in this film I saw changes that made no sense whatsoever.<br /><br />Another thing, there is zero character development. When characters in the story get saved (I won't say who), the book makes it clear that it's a long, soul-searching process. In the film it's quick and artificial. The book is written decently enough where people like Rayford Steele, Buck Williams and Hattie Durham seem real, but in the movie scenarios are consistently given the quick treatment without anything substantial. In another scene where one character gets angry about being left behind (again, I won't say who), it seems artificial.<br /><br />I realize as a Christian it's unedifying for me to say I disliked this film, but I can't in a good conscience recommend a film that I feel was horribly done. Perhaps it would've been better to make the first book into 2-3 films. Either way, Christians need to realize that to be taken seriously as filmmakers, we need to start by putting together a film in a quality way. I realize a lot of effort probably went into Left Behind, but that's the way I see it.\"\n",
      "Review:: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking into data\n",
    "\n",
    "for txt_b, lbl_b in RawTrainDataset.take(1):\n",
    "    print(f\"Review:: {txt_b.numpy()[7]}\")\n",
    "    print(f\"Review:: {lbl_b.numpy()[7]}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the validation set...\n",
    "raw_val_ds = tfk.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=bs,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=s)\n",
    "\n",
    "\n",
    "# getting ready the test set...\n",
    "raw_test_ds = tfk.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=bs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparing Dataset for training**\n",
    "> **Next, you will standardize, tokenize, and vectorize the data using the helpful `tf.keras.layers.TextVectorization` layer.**\n",
    "\n",
    "* **`Standardization` refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset**\n",
    "* **`Tokenization` refers to splitting strings into tokens (for example, splitting a sentence into individual words)**\n",
    "* **`Vectorization` refers to converting tokens into numbers so they can be fed into a neural network**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### **Defining a custom Standardization function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns out, default standardization cannot remove <HTML/> tags, \n",
    "# thus we need to create our own simple one.\n",
    "\n",
    "def cstm_stdfn(data):\n",
    "    lc = tf.strings.lower(data)\n",
    "    # the operation below strinps out basic HTML\n",
    "    formatted = tf.strings.regex_replace(lc, '<br />', '')\n",
    "    return tf.strings.regex_replace(\n",
    "        formatted, '[%s]' % re.escape(string.punctuation), ''\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### **Creating a text-vectorization layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_fea = 10000 # dont know what for\n",
    "slen = 250 # truncate sequences to exact sequence length!!\n",
    "\n",
    "vec_layer = lyrs.TextVectorization(\n",
    "    standardize= cstm_stdfn,\n",
    "    max_tokens= mx_fea,\n",
    "    output_mode= 'int',\n",
    "    output_sequence_length= slen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Next, you will call adapt to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: (None,), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "train_text = RawTrainDataset.map(lambda x, y: x)\n",
    "print(train_text)\n",
    "vec_layer.adapt(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing the above layer with sample data to get insight on text pre-processing results!!\n",
    "\n",
    "def vectorize(text, label):\n",
    "    text = tf.expand_dims(input= text,axis= -1)\n",
    "    return vec_layer(text), label\n",
    "\n",
    "# Retrieving a batch (of 32 reviews and labels) from the dataset\n",
    "(txt_b, lbl_b) = next(iter(RawTrainDataset))\n",
    "first_txt, first_lbl = txt_b[0], lbl_b[0]\n",
    "print(\"Printing out stuff!\")\n",
    "print(f\"First Review:: {first_txt}\")\n",
    "print(f\"Label(encoding):: {first_lbl}\")\n",
    "print(f\"Sentiment:: {RawTrainDataset.class_names[first_lbl]}\")\n",
    "print(\"Vectorized review\", vectorize(first_txt, first_lbl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **You can lookup the token (string) that each integer corresponds to by calling `.get_vocabulary()` on the layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ==> tough\n",
      "2416 ==> speaks\n",
      "10 ==> this\n"
     ]
    }
   ],
   "source": [
    "print(f\"25 ==> {vec_layer.get_vocabulary()[1213]}\")\n",
    "print(f\"2416 ==> {vec_layer.get_vocabulary()[2416]}\")\n",
    "print(f\"10 ==> {vec_layer.get_vocabulary()[10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Applying the TextVectorization layer (created earlier) to the datasets (train, validation, and test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((None, 250), (None,)), types: (tf.int64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "train_ds = RawTrainDataset.map(vectorize)\n",
    "val_ds = raw_val_ds.map(vectorize) \n",
    "test_ds = raw_test_ds.map(vectorize)\n",
    "\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configure Dataset for performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AT)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AT)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create a Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 16\n",
    "\n",
    "model = tfk.Sequential([\n",
    "    lyrs.Embedding(mx_fea + 1, emb_dim),\n",
    "    lyrs.Dropout(0.2),\n",
    "    lyrs.GlobalAveragePooling1D(),\n",
    "    lyrs.Dropout(0.2),\n",
    "    lyrs.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loss function and optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model training and evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plot Model parameters with time(loss, accuracy)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Export Model and Interface on new data** "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "445be380edc556d8a8859931574c9be2b357dc49fbb96280944087ec4ff5e718"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('OCR': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
